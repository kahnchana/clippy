{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c118fe-07ad-4b24-829c-78689bc647b9",
   "metadata": {},
   "source": [
    "# Evaluate CLIPpy on ImageNet (classification) and PASCAL VOC (semantic segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed28bc-5a8f-486e-8cac-fe213b518e19",
   "metadata": {},
   "source": [
    "## Common\n",
    "Set paths to checkpoints and datasets, and load pre-trained CLIPpy model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7060fd7-d896-4d2f-8e38-fd26185a32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7902577f-c5bd-4b8c-9bae-2811dc6b6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_ckpt = \"/raid/kanchana/checkpoints/open_clip/clippy_best/clippy_5k.pt\"\n",
    "inet_path = \"/raid/datasets/img1k_k/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5511f0b-ebc5-4baf-8dd4-5e3e519013de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from open_clip import create_model_and_transforms, get_tokenizer\n",
    "from training.data import get_imagenet\n",
    "from training.zero_shot import zero_shot_eval\n",
    "\n",
    "from utils import DummyArgs\n",
    "\n",
    "\n",
    "tokenizor = get_tokenizer(\"clippy-B-16\")\n",
    "clippy, preprocess_train, preprocess_val = create_model_and_transforms(\n",
    "    \"clippy-B-16\",\n",
    "    precision='amp',\n",
    "    device=\"cuda:0\",\n",
    "    pretrained=pretrained_ckpt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67fc2c-639f-4a6c-9cc9-d30b3a7d9ba9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ImageNet Classification\n",
    "We load the ImageNet dataset and evaluate CLIPpy. Accuracy (top-1 & top-5) are reported.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e88e05d-890d-4b0d-a7f4-e96f47479ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DummyArgs(\n",
    "    imagenet_val=inet_path,\n",
    "    batch_size=64,\n",
    "    workers=1,\n",
    "    distributed=False,\n",
    "    horovod=False,\n",
    "    precision=\"amp\",  # ensure same precision for model (above) and data (here)\n",
    "    zeroshot_frequency=1,\n",
    "    epochs=0,\n",
    "    model=\"clippy-B-16\",\n",
    "    device=\"cuda:0\"  # ensure same device for model (above) and data (here)\n",
    ")\n",
    "\n",
    "dataset = get_imagenet(args, (preprocess_train, preprocess_val), \"val\")\n",
    "data = {\"imagenet-val\": dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4710a0e6-90a3-4c5c-be44-707bb232d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:35<00:00, 28.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 50048/50048 [1:30:20<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imagenet-zeroshot-val-top1': 0.44976, 'imagenet-zeroshot-val-top5': 0.72826}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zero_shot_metrics = zero_shot_eval(clippy, data, 1, args)\n",
    "print(zero_shot_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4383edf-6fb0-4e4f-b288-d800fd6b8011",
   "metadata": {},
   "source": [
    "## PASCAL VOC Semantic Segmentation\n",
    "Online eval script for fast approximate evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3e970d-2f72-40e3-84ff-3333f6c310ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.datasets import PascalDataset\n",
    "from evaluation.segmentation import PascalMIoU, PascalEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b5acadb-42a0-4d61-a036-eff2f21ac58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DummyArgs(\n",
    "    batch_size=64,\n",
    "    workers=1,\n",
    "    model=\"clippy-B-16\",\n",
    "    device=\"cuda:0\",\n",
    "    precision=\"amp\"  # ensure same precision for model (above) and data (here)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb2d88d-9566-4d8d-bc11-d465ffde7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/raid/datasets/pascal_voc/VOC2012\"\n",
    "pascal_dataset = PascalDataset(data_dir, transform=preprocess_val)\n",
    "\n",
    "metric = PascalMIoU()\n",
    "evaluator = PascalEvaluator(model=clippy, dataset=pascal_dataset, metric=metric, opts=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601810f7-86d3-4423-8718-5851317e07e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1472/1472 [04:12<00:00,  5.82it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8fba6ff-7944-4502-9f36-76212dc2056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mIoU': 0.4210667844036943, 'per_class': [0.7725213189576166, 0.3466781139113337, 0.2781234242346894, 0.38985045635578225, 0.38366327467925143, 0.36911746718162414, 0.5677382659622576, 0.5011231132193903, 0.6720588265609756, 0.16085995911895945, 0.5352997603574671, 0.27677756803172054, 0.6050781162315146, 0.5568816202049942, 0.5501736274433078, 0.1500900159722362, 0.2045999185819136, 0.5984697807708667, 0.2853137003254783, 0.44365273885047407, 0.1943314055257269]}\n"
     ]
    }
   ],
   "source": [
    "res = evaluator.metric.compute()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc015d0-1819-4403-8b8e-4a9b2436961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from utils import *\n",
    "\n",
    "def IOU(a, b):\n",
    "    assert a.shape == b.shape and len(a.shape) == 2\n",
    "    return np.count_nonzero(np.logical_and(a, b)) / np.count_nonzero(np.logical_or(a, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62b824-e6b8-4fc4-a80a-61794d1fef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/kanchana/repo/temp_peekaboo\"\n",
    "\n",
    "data_path = \"/nfs/ws1/datasets/RefVOC\"\n",
    "file_list = f\"{data_path}/refvoc_files.txt\"\n",
    "\n",
    "with open(file_list, \"r\") as fo:\n",
    "    files = fo.readlines()\n",
    "files = [f\"cropped-{x.strip()}.jpg\" for x in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9bea9d-dbf3-43ac-adaf-eef4278acaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    assert os.path.exists(f\"{data_path}/{f}\"), f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce340bf2-dfdc-49f3-bc35-9d126083895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, file_path in tqdm(enumerate(files)):\n",
    "    im = Image.open(f\"{data_path}/{file_path}\")\n",
    "    im_orig = im\n",
    "    im = preprocess_val(im)\n",
    "    im = im.unsqueeze(0).cuda()\n",
    "    \n",
    "    image_features = clippy.encode_image(im, normalize=True, pool=False)[:, 1:]\n",
    "    similarity = get_similarity(image_features, evaluator.class_embeddings, (224, 224), do_argmax=True)\n",
    "    res = similarity[0, 0, :, :]\n",
    "    res[res > 20] = 0\n",
    "    \n",
    "    # cv.imwrite(f\"../notebooks/temp.png\", res.to(torch.uint8).numpy())\n",
    "    cv.imwrite(f\"{save_path}/{file_path[8:].replace('jpg', 'png')}\", res.to(torch.uint8).numpy())\n",
    "    # cv2.imwrite(f\"{target}/{last_name.replace(\"jpg\", \"png\")}' if dataset == 'coco' else f'{last_name}.png'), res)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968e380-7721-42d2-839c-e4a887859cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"cropped-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620f4d3-8e7b-4610-a43f-8c37b7dc9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3d3f6-ddd1-4d1e-b999-2017f29b028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28674e-4bec-4428-9368-4e4a61a6fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(res.to(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be27702-fe98-46ce-b319-b3426681d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d8aa7-2145-4f7a-adcc-caf83c965257",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/kanchana/repo/temp_peekaboo/image-{}.png\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
